@misc{mcinnes2020umapuniformmanifoldapproximation,
      title={UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction}, 
      author={Leland McInnes and John Healy and James Melville},
      year={2020},
      eprint={1802.03426},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1802.03426}, 
}

@article{hdbscan,
author = {McInnes, Leland and Healy, John and Astels, Steve},
year = {2017},
month = {03},
pages = {},
title = {hdbscan: Hierarchical density based clustering},
volume = {2},
journal = {The Journal of Open Source Software},
doi = {10.21105/joss.00205}
}
@article{Zhang1997,
author={Zhang, Tian
and Ramakrishnan, Raghu
and Livny, Miron},
title={BIRCH: A New Data Clustering Algorithm and Its Applications},
journal={Data Mining and Knowledge Discovery},
year={1997},
month={Jun},
day={01},
volume={1},
number={2},
pages={141-182},
abstract={Data clustering is an important technique for exploratory data analysis, and has been studied for several years. It has been shown to be useful in many practical domains such as data classification and image processing. Recently, there has been a growing emphasis on exploratory analysis of very large datasets to discover useful patterns and/or correlations among attributes. This is called data mining, and data clustering is regarded as a particular branch. However existing data clustering methods do not adequately address the problem of processing large datasets with a limited amount of resources (e.g., memory and cpu cycles). So as the dataset size increases, they do not scale up well in terms of memory requirement, running time, and result quality.},
issn={1573-756X},
doi={10.1023/A:1009783824328},
url={https://doi.org/10.1023/A:1009783824328}
}

@misc{qwen_embedding,
  title        = {Qwen3-Embedding-8B: A Versatile Text Embedding Model},
  author       = {{Qwen Team}},
  year         = {2025},
  publisher    = {Hugging Face},
  howpublished = {\url{https://huggingface.co/Qwen/Qwen3-Embedding-8B}},
  note         = {Accessed: 2025-12-19}
}

@misc{ollama,
  title        = {Ollama: Get up and running with Llama 3.2, Mistral, Gemma 2, and other large language models},
  author       = {{Ollama}},
  year         = {2025},
  howpublished = {\url{https://github.com/ollama/ollama}},
  note         = {Software version 0.5.x}
}

@inproceedings{wolf-etal-2020-transformers,
  title     = {Transformers: State-of-the-Art Natural Language Processing},
  author    = {Wolf, Thomas  and  Debut, Lysandre  and  Sanh, Victor  and  Chaumond, Julien  and  Delangue, Clement  and  Moi, Anthony  and  Cistac, Pierric  and  Rault, Tim  and  Louf, Remi  and  Funtowicz, Morgan  and  Davison, Joe  and  Shleifer, Sam  and  von Platen, Patrick  and  Ma, Clara  and  Jernite, Yacine  and  Plu, Julien  and  Xu, Canwen  and  Le Scao, Teven  and  Gugger, Sylvain  and  Drame, Mariama  and  Lhoest, Quentin  and  Rush, Alexander},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
  month     = oct,
  year      = {2020},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://www.aclweb.org/anthology/2020.emnlp-demos.6},
  pages     = {38--45}
}

@software{hdf5,
  title  = {Hierarchical Data Format 5 (HDF5)},
  author = {{The HDF Group}},
  year   = {1997-2025},
  url    = {https://www.hdfgroup.org/HDF5/},
  note   = {Champaign, IL, USA}
}

@inproceedings{optuna_2019,
	title={Optuna: A Next-generation Hyperparameter Optimization Framework},
	author={Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
	booktitle={Proceedings of the 25th {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data Mining},
	year={2019}
}

@article{Calinski01011974,
author = {T. Caliński and J Harabasz},
title = {A dendrite method for cluster analysis},
journal = {Communications in Statistics},
volume = {3},
number = {1},
pages = {1--27},
year = {1974},
publisher = {Taylor \& Francis},
doi = {10.1080/03610927408827101},


URL = { 
    
    
        https://www.tandfonline.com/doi/abs/10.1080/03610927408827101
    

},
eprint = { 
    
    
        https://www.tandfonline.com/doi/pdf/10.1080/03610927408827101
    

}

}

@inbook{Schubert_2021,
   title={Accelerating Spherical k-Means},
   ISBN={9783030896577},
   ISSN={1611-3349},
   url={http://dx.doi.org/10.1007/978-3-030-89657-7_17},
   DOI={10.1007/978-3-030-89657-7_17},
   booktitle={Similarity Search and Applications},
   publisher={Springer International Publishing},
   author={Schubert, Erich and Lang, Andreas and Feher, Gloria},
   year={2021},
   pages={217–231} }

@article{JSSv050i10,
 title={Spherical k-Means Clustering},
 volume={50},
 url={https://www.jstatsoft.org/index.php/jss/article/view/v050i10},
 doi={10.18637/jss.v050.i10},
 abstract={Clustering text documents is a fundamental task in modern data analysis, requiring approaches which perform well both in terms of solution quality and computational efficiency. Spherical k-means clustering is one approach to address both issues, employing cosine dissimilarities to perform prototype-based partitioning of term weight representations of the documents. This paper presents the theory underlying the standard spherical k-means problem and suitable extensions, and introduces the R extension package &amp;lt;b&amp;gt;skmeans&amp;lt;/b&amp;gt; which provides a computational environment for spherical k-means clustering featuring several solvers: a fixed-point and genetic algorithm, and interfaces to two external solvers (CLUTO and Gmeans). Performance of these solvers is investigated by means of a large scale benchmark experiment.},
 number={10},
 journal={Journal of Statistical Software},
 author={Hornik, Kurt and Feinerer, Ingo and Kober, Martin and Buchta, Christian},
 year={2012},
 pages={1–22}
}

@article{YANG2024112358,
title = {Dual-stream fusion network with multi-head self-attention for multi-modal fake news detection},
journal = {Applied Soft Computing},
volume = {167},
pages = {112358},
year = {2024},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2024.112358},
url = {https://www.sciencedirect.com/science/article/pii/S1568494624011323},
author = {Yimei Yang and Jinping Liu and Yujun Yang and Lihui Cen},
keywords = {Fake News Detection, Multi-Head Self-Attention, Deep Learning Method, Dual-Stream Network, Social Network},
abstract = {With the rapid advancement of social media platforms like Weibo and WeChat, alongside the emergence of deepfake technologies, tackling fake information has become a major challenge for society and government institutions. To address this, developing efficient and intelligent methods for fake news detection is crucial. This paper introduces a dual-stream fusion network model (DSF-MHSA) based on deep learning, designed to detect fake news across web pages, images, and text. The model tackles issues such as cross-lingual discrepancies, data imbalance, and multimodal information fusion by integrating deep learning models like ERNIE-M, AlexNet, and ShuffleNet, along with three multi-head self-attention mechanisms. It processes textual and image data separately to capture long-range dependencies and global information, enhancing understanding and recognition. A unified multi-head self-attention mechanism then merges these insights to strengthen cross-modal correlation detection. The model is tested on datasets from Twitter, Weibo, and IKCEST-2023, which includes real online news from various social media sources. Results show that the DSF-MHSA model achieves over 90 % accuracy, surpassing traditional models in news detection tasks. This research offers significant practical value for the identification and understanding of news content.}
}

@misc{benbaruch2021asymmetriclossmultilabelclassification,
      title={Asymmetric Loss For Multi-Label Classification}, 
      author={Emanuel Ben-Baruch and Tal Ridnik and Nadav Zamir and Asaf Noy and Itamar Friedman and Matan Protter and Lihi Zelnik-Manor},
      year={2021},
      eprint={2009.14119},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2009.14119}, 
}

@misc{lin2018focallossdenseobject,
      title={Focal Loss for Dense Object Detection}, 
      author={Tsung-Yi Lin and Priya Goyal and Ross Girshick and Kaiming He and Piotr Dollár},
      year={2018},
      eprint={1708.02002},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1708.02002}, 
}
@inproceedings{Chen_2016, series={KDD ’16},
   title={XGBoost: A Scalable Tree Boosting System},
   url={http://dx.doi.org/10.1145/2939672.2939785},
   DOI={10.1145/2939672.2939785},
   booktitle={Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
   publisher={ACM},
   author={Chen, Tianqi and Guestrin, Carlos},
   year={2016},
   month=aug, pages={785–794},
   collection={KDD ’16} }

