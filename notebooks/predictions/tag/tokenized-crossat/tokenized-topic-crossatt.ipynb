{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86440a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = str(Path.cwd().parents[3])\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ddce1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from networks import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,f1_score,multilabel_confusion_matrix\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from torch.optim import Adam, AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau,OneCycleLR\n",
    "import torch\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from torch.nn import BCELoss\n",
    "from tqdm import tqdm\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd5d3b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4b8457",
   "metadata": {},
   "outputs": [],
   "source": [
    "H5_PATH = os.path.join(Path.cwd().parent.parent.parent.parent, 'data', 'stackexchange_embeddings_tokenized.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e40e23e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = pd.read_pickle(os.path.join(Path.cwd().parent,'data','stackexchange_reduced_tags_embeddings.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "beee0b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "num_tags = np.vstack(emb['tags'].apply(len).values.reshape(-1) / 5.0)\n",
    "y = mlb.fit_transform(emb['tags'])\n",
    "indices = emb.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59402b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = LazyDSFDataset(\n",
    "    h5_path=H5_PATH, \n",
    "    num_tags_list=num_tags,\n",
    "    binary_labels=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74a41468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with h5py.File(H5_PATH, \"r\") as f:\n",
    "#     print(f['question_ids'][1040])\n",
    "#     print(f['body_seq'][1040])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c826a7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.9 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size], generator=generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7179b43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=64, \n",
    "    shuffle=True, \n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6976c598",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = full_dataset.binary_labels[train_dataset.indices]\n",
    "num_positives = np.sum(all_labels, axis=0)\n",
    "num_negatives = len(all_labels) - num_positives\n",
    "# pos_weights = torch.tensor(num_negatives / (num_positives + 1e-5), dtype=torch.float32).to(device)\n",
    "pos_weights = torch.tensor(np.sqrt(num_negatives / (num_positives + 1e-5)), dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4940a89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "model = DSF_Sequence_Aware_Classifier(hidden_dim=512, dropout=0.5).to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "scheduler = OneCycleLR(optimizer, max_lr=3e-4, \n",
    "                                          steps_per_epoch=len(train_loader), \n",
    "                                          epochs=num_epochs+1,pct_start=0.3)\n",
    "# criterion = AsymmetricLoss(gamma_neg=2, gamma_pos=1)\n",
    "# criterion = BCEWithLogitsLoss(pos_weight=pos_weights)\n",
    "# Asymmetric loss didn't help here, it underperformed BCEWithLogitsLoss because it overemphasized negative samples\n",
    "# criterion = FocalLoss(alpha=0.25, gamma=2.0)\n",
    "criterion = FocalLossSmooth(alpha=0.25, gamma=2.0, smoothing=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f705ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "mixup_alpha = 0.4\n",
    "\n",
    "def optimize_threshold(probs, targets):\n",
    "    best_t = 0.5\n",
    "    best_f1 = 0.0\n",
    "    \n",
    "    # Search thresholds from 0.1 to 0.9\n",
    "    for t in np.arange(0.1, 0.9, 0.05):\n",
    "        preds = (probs > t).astype(int)\n",
    "        f1 = f1_score(targets, preds, average='macro')\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_t = t\n",
    "    return best_t, best_f1\n",
    "\n",
    "def mixup_data(t_emb, b_emb, n_tags, y, alpha=0.4):\n",
    "    \"\"\"\n",
    "    Applies Mixup to the embeddings and generates paired targets.\n",
    "    \"\"\"\n",
    "    # 1. Generate Mixing Coefficient (Lambda)\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = t_emb.size(0)\n",
    "    \n",
    "    # 2. Generate Permutation Indices\n",
    "    # Use the device of the input tensor to ensure compatibility\n",
    "    index = torch.randperm(batch_size).to(t_emb.device)\n",
    "\n",
    "    # 3. Create Mixed Inputs\n",
    "    # Formula: mixed = λ * original + (1 - λ) * shuffled\n",
    "    mixed_t = lam * t_emb + (1 - lam) * t_emb[index]\n",
    "    mixed_b = lam * b_emb + (1 - lam) * b_emb[index]\n",
    "    mixed_n = lam * n_tags + (1 - lam) * n_tags[index]\n",
    "    \n",
    "    # 4. Return mixed inputs and the two target sets\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_t, mixed_b, mixed_n, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, logits, y_a, y_b, lam):\n",
    "    \"\"\"\n",
    "    Calculates the loss as the weighted sum of losses for both targets.\n",
    "    \"\"\"\n",
    "    return lam * criterion(logits, y_a) + (1 - lam) * criterion(logits, y_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68d9af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 [Train]: 100%|██████████| 1407/1407 [02:25<00:00,  9.66it/s, loss=0.00692]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Results:\n",
      "Optimal Threshold: 0.40 -> New Val F1: 0.4107\n",
      "  Train Loss: 0.0175\n",
      "  Val F1 (Micro): 0.5139\n",
      "  Val F1 (Macro): 0.4107\n",
      "  -> New Best Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100 [Train]: 100%|██████████| 1407/1407 [02:25<00:00,  9.67it/s, loss=0.00664]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Results:\n",
      "Optimal Threshold: 0.40 -> New Val F1: 0.5250\n",
      "  Train Loss: 0.0066\n",
      "  Val F1 (Micro): 0.5890\n",
      "  Val F1 (Macro): 0.5250\n",
      "  -> New Best Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100 [Train]: 100%|██████████| 1407/1407 [02:20<00:00, 10.02it/s, loss=0.00601]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Results:\n",
      "Optimal Threshold: 0.40 -> New Val F1: 0.5596\n",
      "  Train Loss: 0.0058\n",
      "  Val F1 (Micro): 0.6080\n",
      "  Val F1 (Macro): 0.5596\n",
      "  -> New Best Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/100 [Train]: 100%|██████████| 1407/1407 [02:10<00:00, 10.82it/s, loss=0.00303]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Results:\n",
      "Optimal Threshold: 0.35 -> New Val F1: 0.5758\n",
      "  Train Loss: 0.0054\n",
      "  Val F1 (Micro): 0.6004\n",
      "  Val F1 (Macro): 0.5758\n",
      "  -> No improvement. Patience: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/100 [Train]: 100%|██████████| 1407/1407 [02:12<00:00, 10.63it/s, loss=0.00353]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Results:\n",
      "Optimal Threshold: 0.35 -> New Val F1: 0.5864\n",
      "  Train Loss: 0.0052\n",
      "  Val F1 (Micro): 0.6104\n",
      "  Val F1 (Macro): 0.5864\n",
      "  -> New Best Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/100 [Train]: 100%|██████████| 1407/1407 [02:09<00:00, 10.84it/s, loss=0.00461]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Results:\n",
      "Optimal Threshold: 0.35 -> New Val F1: 0.5920\n",
      "  Train Loss: 0.0051\n",
      "  Val F1 (Micro): 0.6147\n",
      "  Val F1 (Macro): 0.5920\n",
      "  -> New Best Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/100 [Train]: 100%|██████████| 1407/1407 [02:12<00:00, 10.59it/s, loss=0.00716]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Results:\n",
      "Optimal Threshold: 0.40 -> New Val F1: 0.5986\n",
      "  Train Loss: 0.0050\n",
      "  Val F1 (Micro): 0.6364\n",
      "  Val F1 (Macro): 0.5986\n",
      "  -> New Best Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/100 [Train]: 100%|██████████| 1407/1407 [02:10<00:00, 10.79it/s, loss=0.00491]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Results:\n",
      "Optimal Threshold: 0.40 -> New Val F1: 0.5979\n",
      "  Train Loss: 0.0049\n",
      "  Val F1 (Micro): 0.6374\n",
      "  Val F1 (Macro): 0.5979\n",
      "  -> New Best Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/100 [Train]: 100%|██████████| 1407/1407 [02:11<00:00, 10.72it/s, loss=0.00422]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Results:\n",
      "Optimal Threshold: 0.35 -> New Val F1: 0.6004\n",
      "  Train Loss: 0.0048\n",
      "  Val F1 (Micro): 0.6224\n",
      "  Val F1 (Macro): 0.6004\n",
      "  -> No improvement. Patience: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/100 [Train]: 100%|██████████| 1407/1407 [02:12<00:00, 10.59it/s, loss=0.00696]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Results:\n",
      "Optimal Threshold: 0.40 -> New Val F1: 0.6066\n",
      "  Train Loss: 0.0048\n",
      "  Val F1 (Micro): 0.6422\n",
      "  Val F1 (Macro): 0.6066\n",
      "  -> New Best Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/100 [Train]: 100%|██████████| 1407/1407 [02:14<00:00, 10.44it/s, loss=0.0034] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Results:\n",
      "Optimal Threshold: 0.40 -> New Val F1: 0.6011\n",
      "  Train Loss: 0.0048\n",
      "  Val F1 (Micro): 0.6391\n",
      "  Val F1 (Macro): 0.6011\n",
      "  -> No improvement. Patience: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/100 [Train]: 100%|██████████| 1407/1407 [02:10<00:00, 10.78it/s, loss=0.00565]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Results:\n",
      "Optimal Threshold: 0.40 -> New Val F1: 0.6067\n",
      "  Train Loss: 0.0047\n",
      "  Val F1 (Micro): 0.6426\n",
      "  Val F1 (Macro): 0.6067\n",
      "  -> New Best Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/100 [Train]: 100%|██████████| 1407/1407 [02:20<00:00, 10.04it/s, loss=0.00627]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Results:\n",
      "Optimal Threshold: 0.40 -> New Val F1: 0.6094\n",
      "  Train Loss: 0.0046\n",
      "  Val F1 (Micro): 0.6432\n",
      "  Val F1 (Macro): 0.6094\n",
      "  -> New Best Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/100 [Train]: 100%|██████████| 1407/1407 [02:18<00:00, 10.17it/s, loss=0.00488]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Results:\n",
      "Optimal Threshold: 0.40 -> New Val F1: 0.6085\n",
      "  Train Loss: 0.0046\n",
      "  Val F1 (Micro): 0.6434\n",
      "  Val F1 (Macro): 0.6085\n",
      "  -> New Best Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/100 [Train]: 100%|██████████| 1407/1407 [02:19<00:00, 10.07it/s, loss=0.00645]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 Results:\n",
      "Optimal Threshold: 0.40 -> New Val F1: 0.6088\n",
      "  Train Loss: 0.0046\n",
      "  Val F1 (Micro): 0.6437\n",
      "  Val F1 (Macro): 0.6088\n",
      "  -> New Best Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/100 [Train]: 100%|██████████| 1407/1407 [02:25<00:00,  9.64it/s, loss=0.00333]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 Results:\n",
      "Optimal Threshold: 0.40 -> New Val F1: 0.6138\n",
      "  Train Loss: 0.0045\n",
      "  Val F1 (Micro): 0.6434\n",
      "  Val F1 (Macro): 0.6138\n",
      "  -> No improvement. Patience: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/100 [Train]: 100%|██████████| 1407/1407 [02:28<00:00,  9.46it/s, loss=0.00423]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 Results:\n",
      "Optimal Threshold: 0.40 -> New Val F1: 0.6103\n",
      "  Train Loss: 0.0045\n",
      "  Val F1 (Micro): 0.6443\n",
      "  Val F1 (Macro): 0.6103\n",
      "  -> New Best Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/100 [Train]: 100%|██████████| 1407/1407 [02:30<00:00,  9.33it/s, loss=0.00891]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 Results:\n",
      "Optimal Threshold: 0.40 -> New Val F1: 0.6171\n",
      "  Train Loss: 0.0045\n",
      "  Val F1 (Micro): 0.6456\n",
      "  Val F1 (Macro): 0.6171\n",
      "  -> New Best Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/100 [Train]: 100%|██████████| 1407/1407 [02:27<00:00,  9.53it/s, loss=0.00402]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 Results:\n",
      "Optimal Threshold: 0.40 -> New Val F1: 0.6143\n",
      "  Train Loss: 0.0044\n",
      "  Val F1 (Micro): 0.6460\n",
      "  Val F1 (Macro): 0.6143\n",
      "  -> New Best Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/100 [Train]: 100%|██████████| 1407/1407 [02:16<00:00, 10.29it/s, loss=0.00476]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 Results:\n",
      "Optimal Threshold: 0.40 -> New Val F1: 0.6176\n",
      "  Train Loss: 0.0044\n",
      "  Val F1 (Micro): 0.6471\n",
      "  Val F1 (Macro): 0.6176\n",
      "  -> New Best Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/100 [Train]: 100%|██████████| 1407/1407 [02:19<00:00, 10.08it/s, loss=0.00524]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 Results:\n",
      "Optimal Threshold: 0.40 -> New Val F1: 0.6152\n",
      "  Train Loss: 0.0044\n",
      "  Val F1 (Micro): 0.6469\n",
      "  Val F1 (Macro): 0.6152\n",
      "  -> No improvement. Patience: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/100 [Train]: 100%|██████████| 1407/1407 [02:14<00:00, 10.46it/s, loss=0.00609]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 Results:\n",
      "Optimal Threshold: 0.40 -> New Val F1: 0.6131\n",
      "  Train Loss: 0.0044\n",
      "  Val F1 (Micro): 0.6452\n",
      "  Val F1 (Macro): 0.6131\n",
      "  -> No improvement. Patience: 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/100 [Train]: 100%|██████████| 1407/1407 [02:21<00:00,  9.92it/s, loss=0.00297]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 Results:\n",
      "Optimal Threshold: 0.40 -> New Val F1: 0.6157\n",
      "  Train Loss: 0.0043\n",
      "  Val F1 (Micro): 0.6444\n",
      "  Val F1 (Macro): 0.6157\n",
      "  -> No improvement. Patience: 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/100 [Train]: 100%|██████████| 1407/1407 [02:20<00:00, 10.02it/s, loss=0.00733]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 Results:\n",
      "Optimal Threshold: 0.40 -> New Val F1: 0.6150\n",
      "  Train Loss: 0.0043\n",
      "  Val F1 (Micro): 0.6441\n",
      "  Val F1 (Macro): 0.6150\n",
      "  -> No improvement. Patience: 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/100 [Train]:  12%|█▏        | 166/1407 [00:16<02:06,  9.80it/s, loss=0.00417]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      7\u001b[39m running_loss = \u001b[32m0.0\u001b[39m\n\u001b[32m      9\u001b[39m train_bar = tqdm(train_loader, desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m [Train]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_tags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_bar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mt_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_emb\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_emb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_emb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbool\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Studies/advanced-data-mining/.venv/lib/python3.13/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Studies/advanced-data-mining/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Studies/advanced-data-mining/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:788\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    787\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    790\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Studies/advanced-data-mining/.venv/lib/python3.13/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_collation:\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.dataset, \u001b[33m\"\u001b[39m\u001b[33m__getitems__\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     52\u001b[39m         data = [\u001b[38;5;28mself\u001b[39m.dataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Studies/advanced-data-mining/.venv/lib/python3.13/site-packages/torch/utils/data/dataset.py:416\u001b[39m, in \u001b[36mSubset.__getitems__\u001b[39m\u001b[34m(self, indices)\u001b[39m\n\u001b[32m    414\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__([\u001b[38;5;28mself\u001b[39m.indices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    415\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m416\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Studies/advanced-data-mining/notebooks/networks/datasets/lazy_tokenized_dataset.py:51\u001b[39m, in \u001b[36mLazyDSFDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     46\u001b[39m body_seq = torch.from_numpy(\u001b[38;5;28mself\u001b[39m.archive[\u001b[33m\"\u001b[39m\u001b[33mbody_seq\u001b[39m\u001b[33m\"\u001b[39m][idx]).float()\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# 2. Load and Invert Mask\u001b[39;00m\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# Saved as: True=Real Word, False=Padding\u001b[39;00m\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# nn.MultiheadAttention expects: True=Padding (Ignore), False=Real Word\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m saved_mask = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43marchive\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbody_mask\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# bool array\u001b[39;00m\n\u001b[32m     52\u001b[39m padding_mask = torch.from_numpy(~saved_mask)  \u001b[38;5;66;03m# Invert: ~True becomes False\u001b[39;00m\n\u001b[32m     54\u001b[39m title_emb = torch.from_numpy(\u001b[38;5;28mself\u001b[39m.archive[\u001b[33m\"\u001b[39m\u001b[33mtitle_emb\u001b[39m\u001b[33m\"\u001b[39m][idx]).float()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/_objects.pyx:54\u001b[39m, in \u001b[36mh5py._objects.with_phil.wrapper\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/_objects.pyx:55\u001b[39m, in \u001b[36mh5py._objects.with_phil.wrapper\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Studies/advanced-data-mining/.venv/lib/python3.13/site-packages/h5py/_hl/dataset.py:918\u001b[39m, in \u001b[36mDataset.__getitem__\u001b[39m\u001b[34m(self, args, new_dtype)\u001b[39m\n\u001b[32m    915\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m selection.nselect == \u001b[32m0\u001b[39m:\n\u001b[32m    916\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m numpy.zeros(selection.array_shape, dtype=new_dtype)\n\u001b[32m--> \u001b[39m\u001b[32m918\u001b[39m arr = numpy.zeros(\u001b[43mselection\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray_shape\u001b[49m, new_dtype, order=\u001b[33m'\u001b[39m\u001b[33mC\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    920\u001b[39m \u001b[38;5;66;03m# Perform the actual read\u001b[39;00m\n\u001b[32m    921\u001b[39m mspace = h5s.create_simple(selection.mshape)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Studies/advanced-data-mining/.venv/lib/python3.13/site-packages/h5py/_hl/selections.py:225\u001b[39m, in \u001b[36mSimpleSelection.array_shape\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    222\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34marray_shape\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    224\u001b[39m     scalar = \u001b[38;5;28mself\u001b[39m._sel[\u001b[32m3\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m225\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(x \u001b[38;5;28;01mfor\u001b[39;00m x, s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscalar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "patience_best_val_f1 = 0.0\n",
    "patience = 10\n",
    "trigger_times = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    train_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
    "\n",
    "    for t_emb, b_emb, padding_mask, n_tags, labels in train_bar:\n",
    "        t_emb, b_emb = t_emb.to(device), b_emb.to(device)\n",
    "        padding_mask = padding_mask.to(device).bool()\n",
    "        n_tags, labels = n_tags.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # --- Mixup Augmentation ---\n",
    "        if epoch < (num_epochs - 5) or trigger_times <= 5:\n",
    "            mixed_t, mixed_b, mixed_n, y_a, y_b, lam = mixup_data(\n",
    "                t_emb, b_emb, n_tags, labels, alpha=mixup_alpha\n",
    "            )\n",
    "            logits, attn_weights = model(mixed_t, mixed_b, padding_mask, mixed_n)\n",
    "            loss = mixup_criterion(criterion, logits, y_a, y_b, lam)\n",
    "        else:          \n",
    "            logits, attn_weights = model(t_emb, b_emb, padding_mask, n_tags)\n",
    "            \n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step() \n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        train_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    \n",
    "    # --- Validation ---\n",
    "    model.eval()\n",
    "    val_targets = []\n",
    "    val_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for t_emb, b_emb,padding_mask, n_tags, labels in val_loader:\n",
    "            t_emb = t_emb.to(device)\n",
    "            b_emb = b_emb.to(device)\n",
    "            padding_mask = padding_mask.to(device)\n",
    "            n_tags = n_tags.to(device)\n",
    "            labels = labels.cpu().numpy()\n",
    "            \n",
    "            logits, attn_weights = model(t_emb, b_emb, padding_mask, n_tags)\n",
    "            \n",
    "            probs = torch.sigmoid(logits).cpu().numpy()\n",
    "            \n",
    "            val_targets.append(labels)\n",
    "            val_probs.append(probs)\n",
    "            \n",
    "    val_targets = np.vstack(val_targets)\n",
    "    val_probs = np.vstack(val_probs)\n",
    "\n",
    "    best_threshold, best_val_f1 = optimize_threshold(val_probs, val_targets)\n",
    "    \n",
    "    val_preds_final = (val_probs > best_threshold).astype(int)\n",
    "    \n",
    "    val_f1_micro = f1_score(val_targets, val_preds_final, average='micro')\n",
    "    val_f1_macro = f1_score(val_targets, val_preds_final, average='macro')\n",
    "    \n",
    "    print(f\"Epoch {epoch+1} Results:\")\n",
    "    print(f\"Optimal Threshold: {best_threshold:.2f} -> New Val F1: {best_val_f1:.4f}\")\n",
    "    print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"  Val F1 (Micro): {val_f1_micro:.4f}\")\n",
    "    print(f\"  Val F1 (Macro): {val_f1_macro:.4f}\")\n",
    "\n",
    "    if val_f1_micro > patience_best_val_f1:\n",
    "        patience_best_val_f1 = val_f1_micro\n",
    "        torch.save(model.state_dict(), \"best_dsf_model.pth\")\n",
    "        trigger_times = 0\n",
    "        print(\"  -> New Best Model Saved!\")\n",
    "    else:\n",
    "        trigger_times += 1\n",
    "        print(f\"  -> No improvement. Patience: {trigger_times}/{patience}\")\n",
    "        if trigger_times >= patience:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2e3496",
   "metadata": {},
   "source": [
    "The model is too strong for our dataset, effectively memorizing the entire dataset.\n",
    "First the ASL was tried, but it underperformed compared to BCEWithLogitsLoss with class weights.\n",
    "Then Focal Loss was tried, which yielded slightly better results than BCEWithLogitsLoss. \n",
    "- Focal loss shows real improvemnt in the threshold optimization step, earlier the optimal threshold was around 0.9, indicating very confident predictions. Now it is around 0.4, indicating more balanced predictions.  \n",
    "\n",
    "Essentially, we've hit *Generalization Ceiling*:\n",
    "- Train Loss (0.002) is near zero. The model has effectively memorized the training data.\n",
    "-  Validation F1 (0.64) is stuck.\n",
    "---\n",
    "Now we see the need to further increase the difficulty of the task by  **Label Smoothing** techniques.\n",
    "- For the *Label Smoothing* we modified the *FocalLoss*\n",
    "\n",
    "After that the loss is 0.003 but f1 didn't improved that much.\n",
    "\n",
    "**Manifold Mixum** resulted in the same performance as before, model memorized everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb593ce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(val_targets, val_preds, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808721c3",
   "metadata": {},
   "source": [
    "Fine tuning the model should help. This time with no mixup for easier task to adapt to the 'pure' data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49e7b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Fine-Tuning Phase (No Mixup)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-Tune Epoch 1/10: 100%|██████████| 1407/1407 [02:44<00:00,  8.57it/s, loss=0.00276]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FT Epoch 1: Threshold 0.30 | F1: 0.6486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-Tune Epoch 2/10: 100%|██████████| 1407/1407 [02:42<00:00,  8.65it/s, loss=0.0021] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FT Epoch 2: Threshold 0.35 | F1: 0.6502\n",
      "-> Improved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-Tune Epoch 3/10: 100%|██████████| 1407/1407 [02:37<00:00,  8.95it/s, loss=0.0031] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FT Epoch 3: Threshold 0.35 | F1: 0.6500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-Tune Epoch 4/10: 100%|██████████| 1407/1407 [02:37<00:00,  8.93it/s, loss=0.00319]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FT Epoch 4: Threshold 0.35 | F1: 0.6503\n",
      "-> Improved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-Tune Epoch 5/10: 100%|██████████| 1407/1407 [02:28<00:00,  9.45it/s, loss=0.00172]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FT Epoch 5: Threshold 0.40 | F1: 0.6505\n",
      "-> Improved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-Tune Epoch 6/10: 100%|██████████| 1407/1407 [02:32<00:00,  9.25it/s, loss=0.00221]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FT Epoch 6: Threshold 0.35 | F1: 0.6494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-Tune Epoch 7/10: 100%|██████████| 1407/1407 [02:28<00:00,  9.48it/s, loss=0.00269]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FT Epoch 7: Threshold 0.35 | F1: 0.6498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-Tune Epoch 8/10: 100%|██████████| 1407/1407 [02:29<00:00,  9.39it/s, loss=0.00241]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FT Epoch 8: Threshold 0.35 | F1: 0.6499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-Tune Epoch 9/10: 100%|██████████| 1407/1407 [02:30<00:00,  9.36it/s, loss=0.00241]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FT Epoch 9: Threshold 0.35 | F1: 0.6488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-Tune Epoch 10/10: 100%|██████████| 1407/1407 [02:32<00:00,  9.25it/s, loss=0.00135]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FT Epoch 10: Threshold 0.40 | F1: 0.6488\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1) \n",
    "criterion = FocalLossSmooth(alpha=0.25, gamma=2.0, smoothing=0.05)\n",
    "\n",
    "ft_epochs = 10\n",
    "# Early stopping parameter took from previous best \n",
    "best_ft_f1 = patience_best_val_f1 \n",
    "\n",
    "for epoch in range(ft_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    train_bar = tqdm(train_loader, desc=f\"Fine-Tune Epoch {epoch+1}/{ft_epochs}\")\n",
    "\n",
    "    for t_emb, b_emb, padding_mask, n_tags, labels in train_bar:\n",
    "        t_emb, b_emb = t_emb.to(device), b_emb.to(device)\n",
    "        padding_mask = padding_mask.to(device).bool()\n",
    "        n_tags, labels = n_tags.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits, _ = model(t_emb, b_emb, padding_mask, n_tags)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        train_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    model.eval()\n",
    "    val_targets = []\n",
    "    val_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for t_emb, b_emb, padding_mask, n_tags, labels in val_loader:\n",
    "            t_emb, b_emb = t_emb.to(device), b_emb.to(device)\n",
    "            padding_mask = padding_mask.to(device)\n",
    "            n_tags, labels = n_tags.to(device), labels.to(device)\n",
    "            labels = labels.cpu().numpy()\n",
    "            \n",
    "            logits, _ = model(t_emb, b_emb, padding_mask, n_tags)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()\n",
    "            \n",
    "            val_targets.append(labels)\n",
    "            val_probs.append(probs)\n",
    "\n",
    "    val_targets = np.vstack(val_targets)\n",
    "    val_probs = np.vstack(val_probs)\n",
    "\n",
    "    best_threshold, current_val_f1 = optimize_threshold(val_probs, val_targets)\n",
    "    \n",
    "    print(f\"FT Epoch {epoch+1}: Threshold {best_threshold:.2f} | F1: {current_val_f1:.4f}\")\n",
    "    \n",
    "    if current_val_f1 > best_ft_f1:\n",
    "        best_ft_f1 = current_val_f1\n",
    "        torch.save(model.state_dict(), \"best_dsf_model_finetuned.pth\")\n",
    "        print(\"-> Improved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
