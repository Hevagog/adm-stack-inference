{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5386b4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hevagog/Studies/advanced-data-mining/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d37000",
   "metadata": {},
   "outputs": [],
   "source": [
    "H5_PATH = os.path.join(Path.cwd().parent.parent, 'data', 'stackexchange_embeddings_tokenized.h5')\n",
    "CSV_PATH = os.path.join(Path.cwd().parent.parent, 'data', 'stackexchange_dataset.csv')\n",
    "MODEL_NAME = 'Qwen/Qwen3-Embedding-8B'\n",
    "MAX_LEN_BODY = 32\n",
    "MAX_LEN_TITLE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cd2e22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output.last_hidden_state # Keep in float32! T_T\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return sum_embeddings / sum_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc7706a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repair():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "    \n",
    "    df = pd.read_csv(CSV_PATH, index_col='question_id')\n",
    "    df = df[~df.index.duplicated()]\n",
    "    \n",
    "    with h5py.File(H5_PATH, 'r+') as f:\n",
    "        body_dset = f['body_seq']\n",
    "        title_dset = f['title_emb']\n",
    "        \n",
    "        bad_indices = []\n",
    "        num_samples = body_dset.shape[0]\n",
    "        chunk_size = 1000\n",
    "        \n",
    "        for i in tqdm(range(0, num_samples, chunk_size), desc=\"Scanning\"):\n",
    "            end = min(i + chunk_size, num_samples)\n",
    "            \n",
    "            b_chunk = body_dset[i:end]\n",
    "            t_chunk = title_dset[i:end]\n",
    "            \n",
    "            bad_mask = ~np.isfinite(b_chunk).all(axis=(1,2)) \n",
    "            bad_mask_t = ~np.isfinite(t_chunk).all(axis=1)\n",
    "            \n",
    "            local_bad = np.where(bad_mask | bad_mask_t)[0]\n",
    "            bad_indices.extend((local_bad + i).tolist())\n",
    "\n",
    "        print(f\"Found {len(bad_indices)} corrupted samples.\")\n",
    "        \n",
    "        if len(bad_indices) == 0:\n",
    "            print(\"No repairs needed!\")\n",
    "            return\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "            \n",
    "        model = AutoModel.from_pretrained(\n",
    "            MODEL_NAME, \n",
    "            trust_remote_code=True,\n",
    "            device_map=\"auto\",          \n",
    "            torch_dtype=torch.float32   \n",
    "        )\n",
    "        model.eval()\n",
    "\n",
    "        for idx in tqdm(bad_indices, desc=\"Fixing\"):\n",
    "            row = df.iloc[idx]\n",
    "            text_body = row['question_text']\n",
    "            text_title = row['title']\n",
    "\n",
    "            inputs_body = tokenizer(\n",
    "                [text_body], return_tensors=\"pt\", padding=\"max_length\", \n",
    "                truncation=True, max_length=MAX_LEN_BODY\n",
    "            ).to(model.device) \n",
    "            \n",
    "            with torch.no_grad():\n",
    "                out_body = model(**inputs_body)\n",
    "                emb_body = out_body.last_hidden_state # float32\n",
    "                \n",
    "                # Clamp to safe float16 range\n",
    "                emb_body = torch.clamp(emb_body, min=-65000, max=65000)\n",
    "                emb_body_np = emb_body.half().cpu().numpy()\n",
    "                \n",
    "            body_dset[idx] = emb_body_np[0]\n",
    "\n",
    "            inputs_title = tokenizer(\n",
    "                [text_title], return_tensors=\"pt\", padding=\"max_length\", \n",
    "                truncation=True, max_length=MAX_LEN_TITLE\n",
    "            ).to(model.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                out_title = model(**inputs_title)\n",
    "                \n",
    "                pooled_title = manual_mean_pooling(out_title, inputs_title.attention_mask)\n",
    "                \n",
    "                pooled_title = torch.clamp(pooled_title, min=-65000, max=65000)\n",
    "                pooled_title_np = pooled_title.half().cpu().numpy()\n",
    "                \n",
    "            title_dset[idx] = pooled_title_np[0]\n",
    "\n",
    "    print(\"Repair complete! No more NaNs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f68d6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory cleaned. Starting repair...\n",
      "Loading CSV...\n",
      "Scanning H5 for corruption...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning: 100%|██████████| 100/100 [00:24<00:00,  4.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 147 corrupted samples.\n",
      "Loading Model in float32 (with CPU offload)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.87s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu and disk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repairing samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fixing: 100%|██████████| 147/147 [12:28<00:00,  5.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repair complete! No more NaNs.\n"
     ]
    }
   ],
   "source": [
    "repair()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
